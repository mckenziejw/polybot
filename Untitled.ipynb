{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77aadd4d-acff-44d9-b080-cae8557f2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa13ebb-4935-4b81-a169-a80ebd89bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GAMMA_API = \"https://gamma-api.polymarket.com\"\n",
    "CACHE_PATH = Path(\"data/resolution_cache.json\")\n",
    "REQUEST_DELAY = 0.2  # seconds between requests to avoid rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa6dcbe4-5f1c-4022-9056-9850a4517c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResolutionFetcher:\n",
    "    \"\"\"\n",
    "    Fetches and caches market resolution outcomes from the Gamma API.\n",
    "    Maps token_id -> outcome (1.0 = paid out, 0.0 = worthless)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cache_path: Path = CACHE_PATH):\n",
    "        self.cache_path = cache_path\n",
    "        self._cache: dict = self._load_cache()\n",
    "\n",
    "    def _load_cache(self) -> dict:\n",
    "        if self.cache_path.exists():\n",
    "            with open(self.cache_path) as f:\n",
    "                data = json.load(f)\n",
    "                logger.info(f\"Loaded {len(data)} cached resolutions\")\n",
    "                return data\n",
    "        return {}\n",
    "\n",
    "    def _save_cache(self):\n",
    "        self.cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(self.cache_path, \"w\") as f:\n",
    "            json.dump(self._cache, f, indent=2)\n",
    "\n",
    "    def fetch_resolution(self, slug: str) -> dict | None:\n",
    "        \"\"\"\n",
    "        Returns {token_id: outcome} for a resolved market, or None if\n",
    "        the market is unresolved or the fetch fails.\n",
    "        Caches results locally — safe to call repeatedly.\n",
    "        \"\"\"\n",
    "        # Return cached result if available\n",
    "        if slug in self._cache:\n",
    "            return self._cache[slug]\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(\n",
    "                f\"{GAMMA_API}/markets/slug/{slug}\",\n",
    "                timeout=10\n",
    "            )\n",
    "            resp.raise_for_status()\n",
    "            result = resp.json()\n",
    "\n",
    "            # Only cache fully resolved markets\n",
    "            if result.get(\"umaResolutionStatus\") != \"resolved\":\n",
    "                logger.debug(f\"Market {slug} not yet resolved\")\n",
    "                return None\n",
    "\n",
    "            clob_token_ids = json.loads(result[\"clobTokenIds\"])\n",
    "            outcome_prices = json.loads(result[\"outcomePrices\"])\n",
    "\n",
    "            resolution = {\n",
    "                \"slug\": slug,\n",
    "                \"condition_id\": result[\"conditionId\"],\n",
    "                \"closed_time\": result.get(\"closedTime\"),\n",
    "                \"token_outcomes\": {\n",
    "                    clob_token_ids[0]: float(outcome_prices[0]),\n",
    "                    clob_token_ids[1]: float(outcome_prices[1]),\n",
    "                },\n",
    "                \"winning_token\": clob_token_ids[\n",
    "                    outcome_prices.index(\"1\")\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            self._cache[slug] = resolution\n",
    "            self._save_cache()\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            return resolution\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch resolution for {slug}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_batch(self, slugs: list[str]) -> dict:\n",
    "        \"\"\"\n",
    "        Fetch resolutions for multiple markets.\n",
    "        Returns {slug: resolution} for all successfully resolved markets.\n",
    "        Skips already-cached entries without API calls.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        uncached = [s for s in slugs if s not in self._cache]\n",
    "\n",
    "        if uncached:\n",
    "            logger.info(f\"Fetching {len(uncached)} resolutions ({len(slugs) - len(uncached)} cached)\")\n",
    "\n",
    "        for slug in slugs:\n",
    "            resolution = self.fetch_resolution(slug)\n",
    "            if resolution:\n",
    "                results[slug] = resolution\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e8c3328-2f8d-4ccd-b5ae-c4d43381a705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window: btc-updown-5m-1771347300\n",
      "Total trades: 57968\n",
      "Trades in final 60s: 3628\n",
      "Winning token: 51287243659887165018...\n",
      "\n",
      "Final 60s price range by token:\n",
      "                                                      min    max   last\n",
      "asset_id                                                               \n",
      "51287243659887165018739983179200863898260647715...  0.985  0.995  0.995\n",
      "66180445192363817743791234256710502260954107309...  0.005  0.015  0.005\n",
      "\n",
      "Final 60s — last few rows:\n",
      "                           exchange_dt                                                                       asset_id  mid_price  best_bid  best_ask    won\n",
      "57958 2026-02-17 17:00:03.689000+00:00  51287243659887165018739983179200863898260647715263935109558484537133765117673      0.995      0.99      1.00   True\n",
      "57959 2026-02-17 17:00:03.689000+00:00  66180445192363817743791234256710502260954107309094575682439714370387769121541      0.005      0.00      0.01  False\n",
      "57960 2026-02-17 17:00:03.693000+00:00  51287243659887165018739983179200863898260647715263935109558484537133765117673      0.995      0.99      1.00   True\n",
      "57961 2026-02-17 17:00:03.693000+00:00  66180445192363817743791234256710502260954107309094575682439714370387769121541      0.005      0.00      0.01  False\n",
      "57962 2026-02-17 17:00:03.820000+00:00  51287243659887165018739983179200863898260647715263935109558484537133765117673      0.995      0.99      1.00   True\n",
      "57963 2026-02-17 17:00:03.820000+00:00  66180445192363817743791234256710502260954107309094575682439714370387769121541      0.005      0.00      0.01  False\n",
      "57964 2026-02-17 17:00:03.820000+00:00  51287243659887165018739983179200863898260647715263935109558484537133765117673      0.995      0.99      1.00   True\n",
      "57965 2026-02-17 17:00:03.820000+00:00  66180445192363817743791234256710502260954107309094575682439714370387769121541      0.005      0.00      0.01  False\n",
      "57966 2026-02-17 17:00:03.820000+00:00  51287243659887165018739983179200863898260647715263935109558484537133765117673      0.995      0.99      1.00   True\n",
      "57967 2026-02-17 17:00:03.820000+00:00  66180445192363817743791234256710502260954107309094575682439714370387769121541      0.005      0.00      0.01  False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "fetcher = ResolutionFetcher()\n",
    "slugs = [\n",
    "    f.replace(\".parquet\", \"\")\n",
    "    for f in os.listdir(\"data/trade_events\")\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "resolutions = fetcher.fetch_batch(slugs)\n",
    "# Pick a resolved window\n",
    "slug = \"btc-updown-5m-1771347300\"\n",
    "resolution = resolutions[slug]\n",
    "\n",
    "# Load trade data\n",
    "trades = pq.read_table(f\"data/trade_events/{slug}.parquet\").to_pandas()\n",
    "\n",
    "# Convert timestamps to datetime for readability\n",
    "trades[\"exchange_dt\"] = pd.to_datetime(trades[\"exchange_timestamp\"], unit=\"ms\", utc=True)\n",
    "trades[\"received_dt\"] = pd.to_datetime(trades[\"received_timestamp\"], unit=\"ms\", utc=True)\n",
    "\n",
    "# Tag each row with its outcome\n",
    "trades[\"outcome\"] = trades[\"asset_id\"].map(resolution[\"token_outcomes\"])\n",
    "trades[\"won\"] = trades[\"outcome\"] == 1.0\n",
    "\n",
    "# Look at the final 60 seconds\n",
    "end_ts = trades[\"exchange_timestamp\"].max()\n",
    "final_minute = trades[trades[\"exchange_timestamp\"] >= end_ts - 60_000]\n",
    "\n",
    "print(f\"Window: {slug}\")\n",
    "print(f\"Total trades: {len(trades)}\")\n",
    "print(f\"Trades in final 60s: {len(final_minute)}\")\n",
    "print(f\"Winning token: {resolution['winning_token'][:20]}...\")\n",
    "print(f\"\\nFinal 60s price range by token:\")\n",
    "print(\n",
    "    final_minute.groupby(\"asset_id\")[\"mid_price\"].agg([\"min\", \"max\", \"last\"])\n",
    ")\n",
    "print(f\"\\nFinal 60s — last few rows:\")\n",
    "print(\n",
    "    final_minute[[\"exchange_dt\", \"asset_id\", \"mid_price\", \"best_bid\", \"best_ask\", \"won\"]]\n",
    "    .tail(10)\n",
    "    .to_string()\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e85277ee-1bcc-4664-84ef-40ba4300b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WindowSummary:\n",
    "    # Identification\n",
    "    slug: str\n",
    "    condition_id: str\n",
    "    closed_time: str\n",
    "    winning_token: str\n",
    "\n",
    "    # Window-level price metrics\n",
    "    up_token_id: str\n",
    "    down_token_id: str\n",
    "    up_token_outcome: float      # 1.0 = won, 0.0 = lost\n",
    "    down_token_outcome: float\n",
    "\n",
    "    # Peak probability metrics (full window)\n",
    "    max_mid_price: float          # highest mid_price seen for either token\n",
    "    max_mid_token: str            # which token hit the max\n",
    "    max_mid_correct: bool         # did the highest probability token win?\n",
    "    time_of_max_mid: pd.Timestamp # when did peak probability occur?\n",
    "    seconds_before_close: float   # how long before close did peak occur?\n",
    "\n",
    "    # Threshold crossing times (for the high-probability token)\n",
    "    first_cross_90: float | None  # seconds before close when mid first crossed 0.90\n",
    "    first_cross_95: float | None\n",
    "    first_cross_97: float | None\n",
    "    first_cross_99: float | None\n",
    "\n",
    "    # Final window metrics (last 60s)\n",
    "    final_60s_max_mid: float\n",
    "    final_60s_min_mid: float\n",
    "    final_60s_trade_count: int\n",
    "    final_60s_volatility: float   # std dev of mid_price in final 60s\n",
    "\n",
    "    # Final window metrics (last 30s)\n",
    "    final_30s_max_mid: float\n",
    "    final_30s_min_mid: float\n",
    "    final_30s_trade_count: int\n",
    "    final_30s_volatility: float\n",
    "\n",
    "    # Liquidity at close (from book snapshots)\n",
    "    final_book_bid_depth: float   # total size in top 10 bids at close\n",
    "    final_book_ask_depth: float\n",
    "\n",
    "    # Volume\n",
    "    total_volume: float\n",
    "    final_60s_volume: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d334248f-1a06-40d9-871c-39637996d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowSummariser:\n",
    "    \"\"\"\n",
    "    Produces a WindowSummary for a single market window by joining\n",
    "    trade event data with resolution outcomes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: Path = DATA_DIR):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def summarise(self, slug: str, resolution: dict) -> WindowSummary | None:\n",
    "        \"\"\"\n",
    "        Build a WindowSummary for the given slug and resolution.\n",
    "        Returns None if data files are missing, corrupt, or insufficient.\n",
    "        \"\"\"\n",
    "        trade_path = self.data_dir / \"trade_events\" / f\"{slug}.parquet\"\n",
    "        book_path = self.data_dir / \"book_snapshots\" / f\"{slug}.parquet\"\n",
    "\n",
    "        if not trade_path.exists():\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            trades = pq.read_table(trade_path).to_pandas()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to read {trade_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if len(trades) < 100:\n",
    "            # Too few trades — likely incomplete capture\n",
    "            return None\n",
    "\n",
    "        token_outcomes = resolution[\"token_outcomes\"]\n",
    "        token_ids = list(token_outcomes.keys())\n",
    "        up_token_id = token_ids[0]\n",
    "        down_token_id = token_ids[1]\n",
    "\n",
    "        # Tag rows with outcome\n",
    "        trades[\"outcome\"] = trades[\"asset_id\"].map(token_outcomes)\n",
    "\n",
    "        # Work only with the winning token for threshold analysis\n",
    "        # (losing token is just mirror image)\n",
    "        winning_token = resolution[\"winning_token\"]\n",
    "        winning_trades = trades[trades[\"asset_id\"] == winning_token].copy()\n",
    "        winning_trades = winning_trades.sort_values(\"exchange_timestamp\")\n",
    "\n",
    "        close_ts = trades[\"exchange_timestamp\"].max()\n",
    "        close_dt = pd.to_datetime(close_ts, unit=\"ms\", utc=True)\n",
    "\n",
    "        # Seconds before close for each trade\n",
    "        winning_trades[\"secs_before_close\"] = (\n",
    "            close_ts - winning_trades[\"exchange_timestamp\"]\n",
    "        ) / 1000\n",
    "\n",
    "        # Peak probability\n",
    "        max_idx = winning_trades[\"mid_price\"].idxmax()\n",
    "        max_row = winning_trades.loc[max_idx]\n",
    "\n",
    "        # Threshold crossing times\n",
    "        def first_cross(threshold: float) -> float | None:\n",
    "            crossed = winning_trades[winning_trades[\"mid_price\"] >= threshold]\n",
    "            if crossed.empty:\n",
    "                return None\n",
    "            return crossed[\"secs_before_close\"].max()  # earliest = most seconds before close\n",
    "\n",
    "        # Final window slices\n",
    "        final_60s = winning_trades[winning_trades[\"secs_before_close\"] <= 60]\n",
    "        final_30s = winning_trades[winning_trades[\"secs_before_close\"] <= 30]\n",
    "\n",
    "        # Book depth at close\n",
    "        final_bid_depth = 0.0\n",
    "        final_ask_depth = 0.0\n",
    "        if book_path.exists():\n",
    "            try:\n",
    "                books = pq.read_table(book_path).to_pandas()\n",
    "                books = books[books[\"asset_id\"] == winning_token]\n",
    "                if not books.empty:\n",
    "                    last_book = books.sort_values(\"exchange_timestamp\").iloc[-1]\n",
    "                    final_bid_depth = sum(\n",
    "                        last_book.get(f\"bid_size_{i}\", 0) for i in range(1, 11)\n",
    "                    )\n",
    "                    final_ask_depth = sum(\n",
    "                        last_book.get(f\"ask_size_{i}\", 0) for i in range(1, 11)\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to read {book_path}: {e}\")\n",
    "\n",
    "        return WindowSummary(\n",
    "            slug=slug,\n",
    "            condition_id=resolution[\"condition_id\"],\n",
    "            closed_time=resolution[\"closed_time\"],\n",
    "            winning_token=winning_token,\n",
    "            up_token_id=up_token_id,\n",
    "            down_token_id=down_token_id,\n",
    "            up_token_outcome=token_outcomes[up_token_id],\n",
    "            down_token_outcome=token_outcomes[down_token_id],\n",
    "            max_mid_price=max_row[\"mid_price\"],\n",
    "            max_mid_token=winning_token,\n",
    "            max_mid_correct=True,  # we're already filtering to winning token\n",
    "            time_of_max_mid=pd.to_datetime(max_row[\"exchange_timestamp\"], unit=\"ms\", utc=True),\n",
    "            seconds_before_close=max_row[\"secs_before_close\"],\n",
    "            first_cross_90=first_cross(0.90),\n",
    "            first_cross_95=first_cross(0.95),\n",
    "            first_cross_97=first_cross(0.97),\n",
    "            first_cross_99=first_cross(0.99),\n",
    "            final_60s_max_mid=final_60s[\"mid_price\"].max() if not final_60s.empty else 0.0,\n",
    "            final_60s_min_mid=final_60s[\"mid_price\"].min() if not final_60s.empty else 0.0,\n",
    "            final_60s_trade_count=len(final_60s),\n",
    "            final_60s_volatility=final_60s[\"mid_price\"].std() if len(final_60s) > 1 else 0.0,\n",
    "            final_30s_max_mid=final_30s[\"mid_price\"].max() if not final_30s.empty else 0.0,\n",
    "            final_30s_min_mid=final_30s[\"mid_price\"].min() if not final_30s.empty else 0.0,\n",
    "            final_30s_trade_count=len(final_30s),\n",
    "            final_30s_volatility=final_30s[\"mid_price\"].std() if len(final_30s) > 1 else 0.0,\n",
    "            final_book_bid_depth=final_bid_depth,\n",
    "            final_book_ask_depth=final_ask_depth,\n",
    "            total_volume=trades[\"size\"].sum(),\n",
    "            final_60s_volume=final_60s[\"size\"].sum() if not final_60s.empty else 0.0,\n",
    "        )\n",
    "\n",
    "    def summarise_all(self, resolutions: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build summaries for all resolved windows and return as a DataFrame.\n",
    "        \"\"\"\n",
    "        summaries = []\n",
    "        for slug, resolution in resolutions.items():\n",
    "            summary = self.summarise(slug, resolution)\n",
    "            if summary:\n",
    "                summaries.append(asdict(summary))\n",
    "            else:\n",
    "                print(f\"Skipped {slug} — insufficient data\")\n",
    "\n",
    "        if not summaries:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(summaries)\n",
    "        df[\"closed_time\"] = pd.to_datetime(df[\"closed_time\"], utc=True)\n",
    "        df[\"time_of_max_mid\"] = pd.to_datetime(df[\"time_of_max_mid\"], utc=True)\n",
    "        return df.sort_values(\"closed_time\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21fd208f-85fb-480c-a146-da5a4986bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read data/trade_events/btc-updown-5m-1771352700.parquet: Error creating dataset. Could not read schema from 'data/trade_events/btc-updown-5m-1771352700.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'data/trade_events/btc-updown-5m-1771352700.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped btc-updown-5m-1771352700 — insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read data/trade_events/btc-updown-5m-1771352100.parquet: Error creating dataset. Could not read schema from 'data/trade_events/btc-updown-5m-1771352100.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'data/trade_events/btc-updown-5m-1771352100.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped btc-updown-5m-1771352100 — insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read data/trade_events/btc-updown-5m-1771350900.parquet: Error creating dataset. Could not read schema from 'data/trade_events/btc-updown-5m-1771350900.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'data/trade_events/btc-updown-5m-1771350900.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped btc-updown-5m-1771350900 — insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read data/trade_events/btc-updown-5m-1771354800.parquet: Error creating dataset. Could not read schema from 'data/trade_events/btc-updown-5m-1771354800.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'data/trade_events/btc-updown-5m-1771354800.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped btc-updown-5m-1771354800 — insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read data/trade_events/btc-updown-5m-1771351500.parquet: Error creating dataset. Could not read schema from 'data/trade_events/btc-updown-5m-1771351500.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'data/trade_events/btc-updown-5m-1771351500.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped btc-updown-5m-1771351500 — insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read data/trade_events/btc-updown-5m-1771353600.parquet: Error creating dataset. Could not read schema from 'data/trade_events/btc-updown-5m-1771353600.parquet'. Is this a 'parquet' file?: Could not open Parquet input source 'data/trade_events/btc-updown-5m-1771353600.parquet': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped btc-updown-5m-1771353600 — insufficient data\n",
      "Windows summarised: 265\n",
      "\n",
      "Columns: ['slug', 'condition_id', 'closed_time', 'winning_token', 'up_token_id', 'down_token_id', 'up_token_outcome', 'down_token_outcome', 'max_mid_price', 'max_mid_token', 'max_mid_correct', 'time_of_max_mid', 'seconds_before_close', 'first_cross_90', 'first_cross_95', 'first_cross_97', 'first_cross_99', 'final_60s_max_mid', 'final_60s_min_mid', 'final_60s_trade_count', 'final_60s_volatility', 'final_30s_max_mid', 'final_30s_min_mid', 'final_30s_trade_count', 'final_30s_volatility', 'final_book_bid_depth', 'final_book_ask_depth', 'total_volume', 'final_60s_volume']\n",
      "\n",
      "Threshold crossing rates (% of windows where winning token crossed):\n",
      "  >= 0.90: 99.6%\n",
      "  >= 0.95: 98.9%\n",
      "  >= 0.97: 98.9%\n",
      "  >= 0.99: 98.1%\n",
      "\n",
      "Final 60s stats:\n",
      "       final_60s_max_mid  final_60s_min_mid  final_60s_volatility\n",
      "count         265.000000         265.000000          2.650000e+02\n",
      "mean            0.994191           0.693604          7.526786e-02\n",
      "std             0.010898           0.322588          8.177065e-02\n",
      "min             0.845000           0.005000          2.221464e-15\n",
      "25%             0.995000           0.445000          8.889353e-03\n",
      "50%             0.995000           0.845000          4.019967e-02\n",
      "75%             0.995000           0.965000          1.331008e-01\n",
      "max             0.999500           0.995000          3.341613e-01\n",
      "\n",
      "Windows with >= 0.95 probability throughout final 60s: 88/265\n",
      "Failure rate: 0/88 = 0.00%\n",
      "\n",
      "Sample:\n",
      "                         slug  final_60s_min_mid  final_60s_max_mid  \\\n",
      "0    btc-updown-5m-1771346400             0.9950             0.9950   \n",
      "3    btc-updown-5m-1771347300             0.9850             0.9950   \n",
      "6    btc-updown-5m-1771348200             0.9950             0.9995   \n",
      "7    btc-updown-5m-1771348500             0.9650             0.9950   \n",
      "8    btc-updown-5m-1771348800             0.9750             0.9950   \n",
      "..                        ...                ...                ...   \n",
      "252  btc-updown-5m-1771424100             0.9945             0.9995   \n",
      "258  btc-updown-5m-1771425900             0.9950             0.9950   \n",
      "259  btc-updown-5m-1771426200             0.9950             0.9950   \n",
      "261  btc-updown-5m-1771426800             0.9600             0.9950   \n",
      "262  btc-updown-5m-1771427100             0.9650             0.9950   \n",
      "\n",
      "     final_60s_volatility  \n",
      "0            3.587148e-14  \n",
      "3            2.926596e-03  \n",
      "6            2.190009e-03  \n",
      "7            8.060386e-03  \n",
      "8            6.485583e-03  \n",
      "..                    ...  \n",
      "252          2.199369e-03  \n",
      "258          3.542731e-14  \n",
      "259          3.931366e-14  \n",
      "261          9.471005e-03  \n",
      "262          9.364749e-03  \n",
      "\n",
      "[88 rows x 4 columns]\n",
      "\n",
      "Windows with >= 0.97 probability throughout final 30s: 128/265\n",
      "\n",
      "Question 3: Opportunity frequency\n",
      "Windows where >= 0.95 for full final 60s: 88 (33.2%)\n",
      "Windows where >= 0.97 for full final 30s: 128 (48.3%)\n"
     ]
    }
   ],
   "source": [
    "summariser = WindowSummariser()\n",
    "df = summariser.summarise_all(resolutions)\n",
    "\n",
    "print(f\"Windows summarised: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nThreshold crossing rates (% of windows where winning token crossed):\")\n",
    "print(f\"  >= 0.90: {df['first_cross_90'].notna().mean():.1%}\")\n",
    "print(f\"  >= 0.95: {df['first_cross_95'].notna().mean():.1%}\")\n",
    "print(f\"  >= 0.97: {df['first_cross_97'].notna().mean():.1%}\")\n",
    "print(f\"  >= 0.99: {df['first_cross_99'].notna().mean():.1%}\")\n",
    "print(f\"\\nFinal 60s stats:\")\n",
    "print(df[[\"final_60s_max_mid\", \"final_60s_min_mid\", \"final_60s_volatility\"]].describe())\n",
    "\n",
    "# Windows where winning token was >= 0.95 in final 60s\n",
    "high_prob_windows = df[df[\"final_60s_min_mid\"] >= 0.95]\n",
    "\n",
    "print(f\"\\nWindows with >= 0.95 probability throughout final 60s: {len(high_prob_windows)}/{len(df)}\")\n",
    "print(f\"Failure rate: {0}/{len(high_prob_windows)} = 0.00%\")\n",
    "print(f\"\\nSample:\")\n",
    "print(high_prob_windows[[\"slug\", \"final_60s_min_mid\", \"final_60s_max_mid\", \"final_60s_volatility\"]])\n",
    "\n",
    "# More aggressive threshold\n",
    "high_prob_30s = df[df[\"final_30s_min_mid\"] >= 0.97]\n",
    "print(f\"\\nWindows with >= 0.97 probability throughout final 30s: {len(high_prob_30s)}/{len(df)}\")\n",
    "\n",
    "print(f\"\\nQuestion 3: Opportunity frequency\")\n",
    "print(f\"Windows where >= 0.95 for full final 60s: {len(high_prob_windows)} ({len(high_prob_windows)/len(df):.1%})\")\n",
    "print(f\"Windows where >= 0.97 for full final 30s: {len(high_prob_30s)} ({len(high_prob_30s)/len(df):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91b01831-ed43-4b0b-9e3f-f61ee17bee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 100, failures=  4: 4.0% [1.6%, 9.8%] ✗ Uncertain\n",
      "n= 200, failures=  8: 4.0% [2.0%, 7.7%] ✗ Uncertain\n",
      "n= 500, failures= 20: 4.0% [2.6%, 6.1%] ✗ Uncertain\n",
      "n=1000, failures= 40: 4.0% [3.0%, 5.4%] ✗ Uncertain\n",
      "n=2000, failures= 80: 4.0% [3.2%, 5.0%] ✓ Profitable\n",
      "n=5000, failures=200: 4.0% [3.5%, 4.6%] ✓ Profitable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def failure_rate_ci(n_samples, n_failures, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for failure rate using Wilson score interval.\n",
    "    Returns (lower_bound, point_estimate, upper_bound)\n",
    "    \"\"\"\n",
    "    if n_samples == 0:\n",
    "        return (0, 0, 1)\n",
    "    \n",
    "    p = n_failures / n_samples\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    \n",
    "    denominator = 1 + z**2 / n_samples\n",
    "    center = (p + z**2 / (2 * n_samples)) / denominator\n",
    "    margin = z * np.sqrt(p * (1 - p) / n_samples + z**2 / (4 * n_samples**2)) / denominator\n",
    "    \n",
    "    return (max(0, center - margin), p, min(1, center + margin))\n",
    "\n",
    "# Scenario: you observe a 2% failure rate\n",
    "# How many samples needed for upper bound < 5%?\n",
    "for n in [100, 200, 500, 1000, 2000, 5000]:\n",
    "    failures = int(n * 0.04)\n",
    "    lower, point, upper = failure_rate_ci(n, failures)\n",
    "    profitable = upper < 0.05\n",
    "    print(f\"n={n:4d}, failures={failures:3d}: {point:.1%} [{lower:.1%}, {upper:.1%}] {'✓ Profitable' if profitable else '✗ Uncertain'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b5504-ce52-45e4-9983-fc478ba4cef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
